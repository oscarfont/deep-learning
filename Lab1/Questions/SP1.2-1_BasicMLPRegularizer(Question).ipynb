{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SP1.2-1_BasicMLPRegularizer(Question).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"wFqFzwARTykk","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","# Whole Class with additions:\n","class Neural_Network(object):\n","    def __init__(self,inputNode=2,hiddenNode = 3, outputNode=1,lmbda = .0001):        \n","        #Define Hyperparameters\n","        self.inputLayerSize = inputNode\n","        self.outputLayerSize = outputNode\n","        self.hiddenLayerSize = hiddenNode\n","        self.Lambda = lmbda\n","        \n","        #Weights (parameters)\n","        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n","        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n","        \n","    def forward(self, X):\n","        #Propogate inputs though network\n","        self.z2 = np.dot(X, self.W1)\n","        self.a2 = self.sigmoid(self.z2)\n","        self.z3 = np.dot(self.a2, self.W2)\n","        yHat = self.sigmoid(self.z3) \n","        return yHat\n","        \n","    def sigmoid(self, z):\n","        #Apply sigmoid activation function to scalar, vector, or matrix\n","        return 1/(1+np.exp(-z))\n","    \n","    def sigmoidPrime(self,z):\n","        #Gradient of sigmoid\n","        return np.exp(-z)/((1+np.exp(-z))**2)\n","    \n","    def loss(self, yHat, y):\n","        #Compute loss for given X,y, use weights already stored in class.\n","        J = 0.5*sum((y-yHat)**2)\n","        return J\n","        \n","    \n","    def loss_reg(self, yHat, y):\n","        #!Task1: compute loss with L1 regularization\n","        J = 0.5*sum((y-yHat)**2) + (self.Lambda/2)*( np.sum(np.absolute(self.W1))+np.sum(np.absolute(self.W2)) )\n","        return J\n","    \n","    def backward(self, X,yHat, y):\n","        self.yHat = yHat\n","        \n","        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n","        dJdW2 = np.dot(self.a2.T, delta3)\n","        \n","        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n","        dJdW1 = np.dot(X.T, delta2)  \n","        return dJdW1,dJdW2#np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n","    \n","    def backward_reg(self, X,yHat, y):\n","        #!Task2: Implement the backward with regularization\n","        self.yHat = yHat \n","        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n","        #Add gradient of regularization term:\n","        dJdW2 = np.dot(self.a2.T, delta3) + self.Lambda*(np.absolute(self.W2))\n","        \n","        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n","        #Add gradient of regularization term:\n","        dJdW1 = np.dot(X.T, delta2) + self.Lambda*(np.absolute(self.W1))\n","        return dJdW1,dJdW2#np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QTAthvQuBoLs","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_reg(NN,X, y,Xt, yt,epoch = 10000,lr = .1):\n","    list_loss = []\n","    list_lossTest = []\n","    \n","    for i in range(epoch):\n","        #Compute derivative with respect to W and W2 for a given X and y:\n","        yHat = NN.forward(X)\n","        \n","        gradW1,gradW2 = NN.backward_reg(X,yHat,y)\n","        #now update the weight using gradient descent\n","        NN.W1 -= gradW1 * lr \n","        NN.W2 -= gradW2 * lr\n","        \n","        if i%100 == 0 : \n","            loss = NN.loss_reg(yHat,y)\n","            print('Loss {}={}'.format(i,loss))\n","            list_loss.append(loss)\n","            \n","            #check the loss of testing \n","            list_lossTest.append(NN.loss(NN.forward(Xt),yt))\n","        \n","    return list_loss,list_lossTest    \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KvVCquBwB3wS","colab_type":"code","outputId":"57c55e90-a693-45b4-b499-e7599f664225","executionInfo":{"status":"ok","timestamp":1556401565841,"user_tz":-120,"elapsed":1443,"user":{"displayName":"OSCAR FONT","photoUrl":"","userId":"15760045532924588009"}},"colab":{"base_uri":"https://localhost:8080/","height":2353}},"cell_type":"code","source":["#Training Data:\n","trainX = np.array(([3,5], [5,1], [10,2], [6,1.5]), dtype=float)\n","trainY = np.array(([75], [82], [93], [70]), dtype=float)\n","\n","#Testing Data:\n","testX = np.array(([4, 5.5], [4.5,1], [9,2.5], [6, 2]), dtype=float)\n","testY = np.array(([70], [89], [85], [75]), dtype=float)\n","\n","#Normalize:\n","trainX = trainX/np.amax(trainX, axis=0)\n","trainY = trainY/100 #Max test score is 100\n","\n","#Normalize by max of training data:\n","testX = testX/np.amax(trainY, axis=0)\n","testY = testY/100 #Max test score is 100\n","\n","#NN = Neural_Network()\n","NN = Neural_Network(lmbda = .0001)\n","\n","print('before ',trainX,trainY,'=',NN.forward(trainX))\n","#Train network with new data:\n","#list_loss,list_loss2 = train(NN,trainX,trainY,testX,testY)\n","\n","list_loss,list_loss2 = train_reg(NN,trainX,trainY,testX,testY)\n","\n","print('before ',trainX,trainY,'=',NN.forward(trainX))\n","\n","#!Task3:Observe the different. Pay attentioin to your lambda value\n","import matplotlib.pyplot as plt\n","plt.plot(list_loss, 'r') #train - red\n","plt.plot(list_loss2, 'b') #test - blue\n","plt.show()"],"execution_count":81,"outputs":[{"output_type":"stream","text":["before  [[0.3 1. ]\n"," [0.5 0.2]\n"," [1.  0.4]\n"," [0.6 0.3]] [[0.75]\n"," [0.82]\n"," [0.93]\n"," [0.7 ]] = [[0.29546857]\n"," [0.2913231 ]\n"," [0.27236352]\n"," [0.28712431]]\n","Loss 0=[0.54476147]\n","Loss 100=[0.02009381]\n","Loss 200=[0.01175338]\n","Loss 300=[0.01120225]\n","Loss 400=[0.0111293]\n","Loss 500=[0.01110154]\n","Loss 600=[0.01107954]\n","Loss 700=[0.01105911]\n","Loss 800=[0.0110397]\n","Loss 900=[0.01102121]\n","Loss 1000=[0.01100357]\n","Loss 1100=[0.01098674]\n","Loss 1200=[0.01097067]\n","Loss 1300=[0.01095534]\n","Loss 1400=[0.01094069]\n","Loss 1500=[0.01092669]\n","Loss 1600=[0.01091331]\n","Loss 1700=[0.01090053]\n","Loss 1800=[0.0108883]\n","Loss 1900=[0.0108766]\n","Loss 2000=[0.0108654]\n","Loss 2100=[0.01085468]\n","Loss 2200=[0.01084441]\n","Loss 2300=[0.01083457]\n","Loss 2400=[0.01082513]\n","Loss 2500=[0.01081608]\n","Loss 2600=[0.01080739]\n","Loss 2700=[0.01079905]\n","Loss 2800=[0.01079104]\n","Loss 2900=[0.01078334]\n","Loss 3000=[0.01077594]\n","Loss 3100=[0.01076882]\n","Loss 3200=[0.01076196]\n","Loss 3300=[0.01075536]\n","Loss 3400=[0.01074899]\n","Loss 3500=[0.01074286]\n","Loss 3600=[0.01073694]\n","Loss 3700=[0.01073123]\n","Loss 3800=[0.01072571]\n","Loss 3900=[0.01072038]\n","Loss 4000=[0.01071522]\n","Loss 4100=[0.01071024]\n","Loss 4200=[0.01070541]\n","Loss 4300=[0.01070074]\n","Loss 4400=[0.01069621]\n","Loss 4500=[0.01069182]\n","Loss 4600=[0.01068756]\n","Loss 4700=[0.01068343]\n","Loss 4800=[0.01067942]\n","Loss 4900=[0.01067552]\n","Loss 5000=[0.01067173]\n","Loss 5100=[0.01066805]\n","Loss 5200=[0.01066446]\n","Loss 5300=[0.01066097]\n","Loss 5400=[0.01065758]\n","Loss 5500=[0.01065427]\n","Loss 5600=[0.01065104]\n","Loss 5700=[0.01064789]\n","Loss 5800=[0.01064483]\n","Loss 5900=[0.01064183]\n","Loss 6000=[0.01063891]\n","Loss 6100=[0.01063605]\n","Loss 6200=[0.01063326]\n","Loss 6300=[0.01063054]\n","Loss 6400=[0.01062787]\n","Loss 6500=[0.01062526]\n","Loss 6600=[0.01062271]\n","Loss 6700=[0.01062022]\n","Loss 6800=[0.01061777]\n","Loss 6900=[0.01061538]\n","Loss 7000=[0.01061303]\n","Loss 7100=[0.01061074]\n","Loss 7200=[0.01060848]\n","Loss 7300=[0.01060628]\n","Loss 7400=[0.01060411]\n","Loss 7500=[0.01060199]\n","Loss 7600=[0.0105999]\n","Loss 7700=[0.01059786]\n","Loss 7800=[0.01059585]\n","Loss 7900=[0.01059388]\n","Loss 8000=[0.01059194]\n","Loss 8100=[0.01059004]\n","Loss 8200=[0.01058817]\n","Loss 8300=[0.01058634]\n","Loss 8400=[0.01058453]\n","Loss 8500=[0.01058276]\n","Loss 8600=[0.01058102]\n","Loss 8700=[0.0105793]\n","Loss 8800=[0.01057762]\n","Loss 8900=[0.01057596]\n","Loss 9000=[0.01057433]\n","Loss 9100=[0.01057273]\n","Loss 9200=[0.01057115]\n","Loss 9300=[0.01056959]\n","Loss 9400=[0.01056806]\n","Loss 9500=[0.01056656]\n","Loss 9600=[0.01056507]\n","Loss 9700=[0.01056361]\n","Loss 9800=[0.01056218]\n","Loss 9900=[0.01056076]\n","before  [[0.3 1. ]\n"," [0.5 0.2]\n"," [1.  0.4]\n"," [0.6 0.3]] [[0.75]\n"," [0.82]\n"," [0.93]\n"," [0.7 ]] = [[0.74436356]\n"," [0.80362316]\n"," [0.8402117 ]\n"," [0.81000733]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npBoy3CWphv4/VasMzkmfTkwAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"nyqJ3CE576zU","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}