{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SP1.2-3_BasicMLPPytorch(Question).ipynb","version":"0.3.2","provenance":[{"file_id":"1HUwagaHuz5PcHCo4Ez_JDJEql7yS0K8-","timestamp":1554850581214},{"file_id":"1JcBBryswTVd8j9Nt0DY67TLpQ0KOJs6j","timestamp":1554850555093}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"wFqFzwARTykk","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import torch.nn as nn\n","import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p5Fryl2XCVPo","colab_type":"code","colab":{}},"cell_type":"code","source":["class Neural_Network(nn.Module):\n","    def __init__(self,inputNode=2,hiddenNode = 3, outputNode=1,lmbda  = 0):   \n","        super(Neural_Network, self).__init__()     \n","        #Define Hyperparameters\n","        self.inputLayerSize = inputNode\n","        self.outputLayerSize = outputNode\n","        self.hiddenLayerSize = hiddenNode\n","        self.lmbda = lmbda\n","        \n","        # weights\n","        self.Linear1 = nn.Linear(self.inputLayerSize, self.hiddenLayerSize)\n","        self.Linear2 = nn.Linear(self.hiddenLayerSize, self.outputLayerSize)\n","        \n","    def forward(self, X):\n","        self.z2 = self.Linear1(X) # 3 X 3 \".dot\" does not broadcast in PyTorch\n","        self.a2 = self.sigmoid(self.z2) # activation function\n","        self.z3 = self.Linear2(self.a2)\n","        y_hat = self.sigmoid(self.z3) # final activation function\n","        return y_hat \n","        \n","    def sigmoid(self, z):\n","        #Apply sigmoid activation function to scalar, vector, or matrix\n","        return 1/(1+torch.exp(-z))\n","    \n","    def loss(self, yHat, y):\n","        J = 0.5*sum((y-yHat)**2)\n","        #!Task 1, acquire the l2_reg term for all model parameters\n","        #remember model parameters can be accessed with model.parameters\n","        l2_reg = 0\n","        for params in self.parameters():\n","            l2_reg += (params**2).sum()\n","        \n","        return J + self.lmbda * l2_reg"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u8MsVzDgCb8U","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(NN,X, y,Xt,yt,epoch = 5000,optimizer = None):\n","    list_loss = []\n","    list_lossTest = []\n","    \n","    for i in range(epoch):\n","        NN.train()\n","        \n","        optimizer.zero_grad()\n","        \n","        yHat = NN.forward(X)\n","        loss = NN.loss(yHat,y)\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","        if i%100 == 0 : \n","            print('Loss {}={}'.format(i,loss.item()))\n","            \n","            list_loss.append(loss)\n","            #check the loss of testing \n","            with torch.no_grad(): \n","                NN.eval()\n","                list_lossTest.append(NN.loss(NN.forward(Xt),yt))\n","        \n","    return list_loss,list_lossTest\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"725SIWP2CfZ4","colab_type":"code","outputId":"9a7c8557-bfac-4108-cb3e-31778bffe8a5","executionInfo":{"status":"ok","timestamp":1556099662910,"user_tz":-120,"elapsed":12692,"user":{"displayName":"OSCAR FONT","photoUrl":"","userId":"15760045532924588009"}},"colab":{"base_uri":"https://localhost:8080/","height":2473}},"cell_type":"code","source":["\n","#Training Data:\n","trainX = np.array(([3,5], [5,1], [10,2], [6,1.5]), dtype=float)\n","trainY = np.array(([75], [82], [93], [70]), dtype=float)\n","\n","#Testing Data:\n","testX = np.array(([4, 5.5], [4.5,1], [9,2.5], [6, 2]), dtype=float)\n","testY = np.array(([70], [89], [85], [75]), dtype=float)\n","\n","#Normalize:\n","trainX = trainX/np.amax(trainX, axis=0)\n","trainY = trainY/100 #Max test score is 100\n","\n","#Normalize by max of training data:\n","testX = testX/np.amax(testX, axis=0)\n","testY = testY/100 #Max test score is 100\n","\n","trainX = torch.from_numpy(trainX).float()\n","trainY = torch.from_numpy(trainY).float()\n","testX = torch.from_numpy(testX).float()\n","testY = torch.from_numpy(testY).float()\n","\n","\n","NN = Neural_Network(lmbda = .001)\n","\n","optimizer = torch.optim.SGD(NN.parameters(), lr=.1,momentum=.09)\n","\n","print('before ',trainX,trainY,'=',NN.forward(trainX))\n","\n","list_loss,list_loss2 = train(NN,trainX,trainY,testX,testY,10000,optimizer)\n","\n","print('after ',trainX,trainY,'=',NN.forward(trainX))\n","\n","#!Task 2: Validate your result, by comparing the loss progression.\n","import matplotlib.pyplot as plt\n","plt.plot(list_loss, 'r') #train loss - red\n","plt.plot(list_loss2, 'b') #test loss - blue\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["before  tensor([[0.3000, 1.0000],\n","        [0.5000, 0.2000],\n","        [1.0000, 0.4000],\n","        [0.6000, 0.3000]]) tensor([[0.7500],\n","        [0.8200],\n","        [0.9300],\n","        [0.7000]]) = tensor([[0.4863],\n","        [0.4768],\n","        [0.4679],\n","        [0.4754]], grad_fn=<MulBackward0>)\n","Loss 0=0.2257380336523056\n","Loss 100=0.015362328849732876\n","Loss 200=0.014931733720004559\n","Loss 300=0.014903557486832142\n","Loss 400=0.014881499111652374\n","Loss 500=0.01485982071608305\n","Loss 600=0.014838358387351036\n","Loss 700=0.014817051589488983\n","Loss 800=0.014795857481658459\n","Loss 900=0.014774725772440434\n","Loss 1000=0.014753592200577259\n","Loss 1100=0.014732398092746735\n","Loss 1200=0.014711137861013412\n","Loss 1300=0.014689705334603786\n","Loss 1400=0.014668110758066177\n","Loss 1500=0.01464628241956234\n","Loss 1600=0.014624196104705334\n","Loss 1700=0.014601788483560085\n","Loss 1800=0.014579027891159058\n","Loss 1900=0.01455586589872837\n","Loss 2000=0.014532282948493958\n","Loss 2100=0.014508233405649662\n","Loss 2200=0.014483681879937649\n","Loss 2300=0.014458569698035717\n","Loss 2400=0.014432880096137524\n","Loss 2500=0.014406586065888405\n","Loss 2600=0.0143796531483531\n","Loss 2700=0.014352019876241684\n","Loss 2800=0.014323694631457329\n","Loss 2900=0.01429461408406496\n","Loss 3000=0.014264763332903385\n","Loss 3100=0.014234120957553387\n","Loss 3200=0.01420263759791851\n","Loss 3300=0.014170292764902115\n","Loss 3400=0.014137067832052708\n","Loss 3500=0.014102953486144543\n","Loss 3600=0.01406787894666195\n","Loss 3700=0.014031859114766121\n","Loss 3800=0.01399486418813467\n","Loss 3900=0.01395687647163868\n","Loss 4000=0.013917870819568634\n","Loss 4100=0.013877855613827705\n","Loss 4200=0.013836784288287163\n","Loss 4300=0.013794667087495327\n","Loss 4400=0.01375149842351675\n","Loss 4500=0.01370724756270647\n","Loss 4600=0.013661944307386875\n","Loss 4700=0.013615543954074383\n","Loss 4800=0.013568096794188023\n","Loss 4900=0.013519566506147385\n","Loss 5000=0.013469986617565155\n","Loss 5100=0.013419357128441334\n","Loss 5200=0.013367690145969391\n","Loss 5300=0.013315009884536266\n","Loss 5400=0.01326133031398058\n","Loss 5500=0.013206684961915016\n","Loss 5600=0.013151101768016815\n","Loss 5700=0.013094602152705193\n","Loss 5800=0.013037227094173431\n","Loss 5900=0.012979019433259964\n","Loss 6000=0.012920030392706394\n","Loss 6100=0.012860291637480259\n","Loss 6200=0.012799864634871483\n","Loss 6300=0.012738802470266819\n","Loss 6400=0.012677164748311043\n","Loss 6500=0.012615024112164974\n","Loss 6600=0.012552425265312195\n","Loss 6700=0.012489454820752144\n","Loss 6800=0.01242617517709732\n","Loss 6900=0.012362672947347164\n","Loss 7000=0.01229900773614645\n","Loss 7100=0.012235278263688087\n","Loss 7200=0.012171540409326553\n","Loss 7300=0.012107903137803078\n","Loss 7400=0.012044411152601242\n","Loss 7500=0.01198117807507515\n","Loss 7600=0.011918283998966217\n","Loss 7700=0.011855789460241795\n","Loss 7800=0.011793788522481918\n","Loss 7900=0.01173234824091196\n","Loss 8000=0.011671549640595913\n","Loss 8100=0.01161146815866232\n","Loss 8200=0.011552167125046253\n","Loss 8300=0.011493708938360214\n","Loss 8400=0.011436159722507\n","Loss 8500=0.011379566043615341\n","Loss 8600=0.01132399495691061\n","Loss 8700=0.01126947533339262\n","Loss 8800=0.011216058395802975\n","Loss 8900=0.011163778603076935\n","Loss 9000=0.01111266203224659\n","Loss 9100=0.011062726378440857\n","Loss 9200=0.011014003306627274\n","Loss 9300=0.010966493748128414\n","Loss 9400=0.010920219123363495\n","Loss 9500=0.010875157080590725\n","Loss 9600=0.010831321589648724\n","Loss 9700=0.010788707993924618\n","Loss 9800=0.010747285559773445\n","Loss 9900=0.01070705521851778\n","after  tensor([[0.3000, 1.0000],\n","        [0.5000, 0.2000],\n","        [1.0000, 0.4000],\n","        [0.6000, 0.3000]]) tensor([[0.7500],\n","        [0.8200],\n","        [0.9300],\n","        [0.7000]]) = tensor([[0.7546],\n","        [0.7978],\n","        [0.8362],\n","        [0.8048]], grad_fn=<MulBackward0>)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wCXcNlXMfs4AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]}]}